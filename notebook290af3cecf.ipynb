{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70068,"databundleVersionId":7878506,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime, timedelta\nfrom IPython.display import clear_output\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-18T23:05:29.441801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_data_season = {\n    'M': 2003,\n    'W': 2009\n}\nclassifiers = {\n    'M': RandomForestClassifier(max_depth=80, max_features='sqrt', min_samples_leaf=4,\n                       n_estimators=1600, random_state=0),\n    'W': RandomForestClassifier(max_depth=100, max_features=None, min_samples_leaf=4,\n                       min_samples_split=5, n_estimators=2000, random_state=0)\n}\ncolumns = [\n            'Season', 'TeamA', 'TeamB', 'TeamNameA', 'TeamNameB', \n            'ASeedNum', 'ALastTournPct', 'ARegSznPct',\n            'APts', 'APtsOpp', 'AFG', 'AFG3', 'AFT', 'AAst', 'ATO', 'AOR', 'ADR', 'AStl', 'ABlk', 'APF',\n            'BSeedNum', 'BLastTournPct', 'BRegSznPct',\n            'BPts', 'BPtsOpp', 'BFG', 'BFG3', 'BFT', 'BAst', 'BTO', 'BOR', 'BDR', 'BStl', 'BBlk', 'BPF',\n            'Winner'\n        ]\ncurrent_season = 2024","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_teams_info(gender):\n    teams_file = f'/kaggle/input/march-machine-learning-mania-2024/{gender}Teams.csv'\n    teams_df = pd.read_csv(teams_file)\n    display(teams_df)\n    return teams_df\n\nm_teams_info_df = get_teams_info('M')\nw_teams_info_df = get_teams_info('W')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_seed_number(seed):\n    if 'a' in seed or 'b' in seed:\n        return 17\n    return int(seed[1:])\n\ndef generate_numeric_cols(df, team):\n    df[f'{team}FG'] = (df[f'{team}FGM'] * 100) / df[f'{team}FGA']\n    df[f'{team}FG3'] = (df[f'{team}FGM3'] * 100) / df[f'{team}FGA3']\n    df[f'{team}FT'] = (df[f'{team}FTM'] * 100) / df[f'{team}FTA']\n\ndef get_files_by_competition(gender):\n    # Getting teams df\n    teams_file = f'/kaggle/input/march-machine-learning-mania-2024/{gender}TeamConferences.csv'\n    teams_df = pd.read_csv(teams_file)\n    display(teams_df)\n    \n    # Getting regular season games df\n    reg_szn_file = f'/kaggle/input/march-machine-learning-mania-2024/{gender}RegularSeasonDetailedResults.csv'\n    reg_szn_df = pd.read_csv(reg_szn_file)\n    generate_numeric_cols(reg_szn_df, 'W')\n    generate_numeric_cols(reg_szn_df, 'L')\n    display(reg_szn_df)\n    \n    # Getting tournament season games df\n    tourn_file = f'/kaggle/input/march-machine-learning-mania-2024/{gender}NCAATourneyCompactResults.csv'\n    tourn_df = pd.read_csv(tourn_file)\n    display(tourn_df)\n    \n    # Getting seeding df\n    seed_file = f'/kaggle/input/march-machine-learning-mania-2024/{gender}NCAATourneySeeds.csv'\n    seed_df = pd.read_csv(seed_file)\n    seed_df['SeedNum'] = seed_df.Seed.apply(lambda x: get_seed_number(x))\n    display(seed_df)\n    \n    return teams_df, reg_szn_df, tourn_df, seed_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_reg_szn_stats(season, team, reg_szn_df):\n    w_games = reg_szn_df[reg_szn_df['WTeamID'] == team].rename(columns = {\n        'WFG': 'FG', 'WFG3': 'FG3', 'WFT': 'FT', 'WAst': 'Ast', 'WTO': 'TO', 'WOR': 'OR', 'WDR': 'DR', 'WStl': 'Stl', 'WBlk': 'Blk', 'WPF': 'PF',\n        'WScore': 'Score',\n        \n        'LFG': 'OppFG', 'LFG3': 'OppFG3', 'LFT': 'OppFT', 'LAst': 'OppAst', 'LTO': 'OppTO', 'LOR': 'OppOR', 'LDR': 'OppDR', 'LStl': 'OppStl', 'LBlk': 'OppBlk', 'LPF': 'OppPF',\n        'LScore': 'OppScore',\n        })\n    w_games['Won'] = 1\n    \n    l_games = reg_szn_df[reg_szn_df['LTeamID'] == team].rename(columns = {\n        'LFG': 'FG', 'LFG3': 'FG3', 'LFT': 'FT', 'LAst': 'Ast', 'LTO': 'TO', 'LOR': 'OR', 'LDR': 'DR', 'LStl': 'Stl', 'LBlk': 'Blk', 'LPF': 'PF',\n        'LScore': 'Score',\n        \n        'WFG': 'OppFG', 'WFG3': 'OppFG3', 'WFT': 'OppFT', 'WAst': 'OppAst', 'WTO': 'OppTO', 'WOR': 'OppOR', 'WDR': 'OppDR', 'WStl': 'OppStl', 'WBlk': 'OppBlk', 'WPF': 'OppPF',\n        'WScore': 'OppScore',\n        })\n    l_games['Won'] = 0\n    \n    games = pd.concat([w_games, l_games], axis=0, ignore_index=True)\n    \n    reg_szn_pct = (len(w_games) * 100) / len(games)\n    \n    return [   \n               reg_szn_pct, \n               games['Score'].mean(), games['OppScore'].mean(), games['FG'].mean(), games['FG3'].mean(), games['FT'].mean(), games['Ast'].mean(), games['TO'].mean(), \n               games['OR'].mean(), games['DR'].mean(), games['Stl'].mean(), games['Blk'].mean(), games['PF'].mean()\n           ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_last_tourn_pct(season, team, tourn_df):\n    wins = len(tourn_df[(tourn_df['Season'] == season-1) & (tourn_df['WTeamID'] == team)])\n    total_games = len(tourn_df[(tourn_df['Season'] == season-1) & ((tourn_df['WTeamID'] == team) | (tourn_df['LTeamID'] == team))])\n    \n    if not total_games: return 0\n    return (wins * 100) / total_games","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_team_name(team_id, teams_df):\n    return teams_df[teams_df['TeamID'] == team_id].reset_index().loc[0, 'TeamName']\n\ndef get_game_stats(gender, season, team_a, team_b, reg_szn_df, seed_df, tourn_df):\n    a_reg_szn_stats = get_reg_szn_stats(season, team_a, reg_szn_df)\n    b_reg_szn_stats = get_reg_szn_stats(season, team_b, reg_szn_df)\n\n    try:\n        a_seed = seed_df[(seed_df['Season'] == season) & (seed_df['TeamID'] == team_a)].reset_index().loc[0, 'SeedNum']\n    except:\n        a_seed = 24\n    try:\n        b_seed = seed_df[(seed_df['Season'] == season) & (seed_df['TeamID'] == team_b)].reset_index().loc[0, 'SeedNum']\n    except:\n        b_seed = 24\n\n    a_last_tourn_pct = get_last_tourn_pct(season, team_a, tourn_df)\n    b_last_tourn_pct = get_last_tourn_pct(season, team_b, tourn_df)\n\n    stats_a = [a_seed, a_last_tourn_pct] + (a_reg_szn_stats)\n    stats_b = [b_seed, b_last_tourn_pct] + (b_reg_szn_stats)\n    \n    teams_info_df = m_teams_info_df if gender == 'M' else w_teams_info_df\n    team_a_name = set_team_name(team_a, teams_info_df)\n    team_b_name = set_team_name(team_b, teams_info_df)\n    \n    return [season, team_a, team_b, team_a_name, team_b_name] + stats_a + stats_b\n\ndef build_dataset(gender, start_data_season, reg_szn_df, tourn_df, seed_df, end_season):\n    data = []\n\n    for season in range(start_data_season + 1, end_season + 1):\n        tourney_games = tourn_df[tourn_df['Season'] == season].reset_index(drop=True)\n        for idx, g in tourney_games.iterrows():\n\n            clear_output(wait=True)\n            print(f\"{season}: {idx}/{len(tourney_games)}\")\n\n            team_a = min([g['WTeamID'], g['LTeamID']])\n            team_b = max([g['WTeamID'], g['LTeamID']])\n\n            if team_a == g['WTeamID']:\n                winner = 'A'\n                team_a_score = g['WScore']\n                team_b_score = g['LScore']\n            else:\n                winner = 'B'\n                team_a_score = g['LScore']\n                team_b_score = g['WScore']\n\n            print(f\"{team_a} x {team_b}\")\n\n            game_stats = get_game_stats(gender, season, team_a, team_b, reg_szn_df, seed_df, tourn_df)\n\n            data.append(game_stats + [winner])\n\n    data_df = pd.DataFrame(data, columns=columns)\n    display(data_df)\n    \n    return data_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(x_train, y_train, gender):\n    classifier = classifiers[gender]\n    classifier.fit(x_train, y_train)\n    return classifier\n\ndef predict_seasons(gender, start_data_season, data_df, detailed_results):\n\n    acc_sum = 0\n    seasons_count = 0\n\n    for season in range(start_data_season + 2, current_season + 1):\n        data_train = data_df[(data_df['Season'] < season)].reset_index(drop=True)\n        data_test = data_df[(data_df['Season'] == season)].reset_index(drop=True)\n\n        if not len(data_test):\n            continue\n\n        x_train = data_train.drop(['TeamNameA', 'TeamNameB', 'Winner'], axis=1)\n        y_train = data_train.loc[:, 'Winner']\n            \n        classifier = train_model(x_train, y_train, gender)\n\n        x_test = data_test.drop(['TeamNameA', 'TeamNameB', 'Winner'], axis=1)\n        y_test = data_test.loc[:, 'Winner']\n\n        predictions = classifier.predict(x_test)\n        \n        acc = accuracy_score(y_test, predictions)\n        acc_sum += acc\n        seasons_count += 1\n        \n        if detailed_results:\n            print(f'\\nResults for {gender} season {season}:')\n            print('Accuracy predictions:', acc)\n\n            cm = confusion_matrix(y_test, predictions)\n            print('Confusion matrix:')\n            cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n            cm_disp.plot()\n            plt.show()\n\n    print(f'\\n\\n{gender} Mean Accuracy: {acc_sum/seasons_count}')\n    \n    return classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_current_predictions(gender, classifier, reg_szn_df, tourn_df, seed_df):\n    data = []\n    teams = seed_df[seed_df['Season'] == season].TeamID.unique()\n    teams.sort()\n    \n    for idx, team_a in enumerate(teams):\n        for idx_b in range(idx + 1, len(teams)):\n            team_b = teams[idx_b]\n            \n            clear_output(wait=True)\n            print(f\"{idx}/{len(teams)}\")\n            print(f\"{team_a} x {team_b}\")\n            \n            game_stats = get_game_stats(gender, season, team_a, team_b, reg_szn_df, seed_df, tourn_df)\n            data.append(game_stats)\n            \n    data_df = pd.DataFrame(data, columns=columns[:-1])\n    data_to_predict = data_df.drop(['TeamNameA', 'TeamNameB'], axis=1)\n    \n    probs = classifier.predict_proba(data_to_predict)\n    pred = classifier.predict(data_to_predict)\n\n    data_df['AProb'] = probs[:, 0]\n    data_df['BProb'] = probs[:, 1]\n    data_df['Pred'] = pred\n    \n    return data_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pipeline(gender, detailed_results=False):\n    teams_df, reg_szn_df, tourn_df, seed_df = get_files_by_competition(gender)\n    data_df = build_dataset(gender, start_data_season[gender], reg_szn_df, tourn_df, seed_df, current_season)\n    classifier = predict_seasons(gender, start_data_season[gender], data_df, detailed_results)\n    current_szn_df = get_current_predictions(gender, classifier, reg_szn_df, tourn_df, seed_df)\n    display(current_szn_df)\n    return current_szn_df, teams_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_current_szn_df, m_teams_df = pipeline('M')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_current_szn_df, w_teams_df = pipeline('W')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_game_id(row):\n    return f\"{row['Season']}_{row['TeamA']}_{row['TeamB']}\"\n\ndef set_impossible_games_probs(gender, current_szn_df, teams_df):\n    teams_a = current_szn_df['TeamA'].unique().tolist()\n    teams_b = current_szn_df['TeamB'].unique().tolist()\n    tourney_teams = set(teams_a + teams_b)\n\n    teams = teams_df[teams_df['Season'] == season]['TeamID'].unique().tolist()\n\n    data = []\n\n    for idx, team_a in enumerate(teams):\n        for idx_b in range(idx + 1, len(teams)):\n            team_b = teams[idx_b]\n\n            if team_a in tourney_teams and team_b in tourney_teams:\n                continue\n                \n            teams_info_df = m_teams_info_df if gender == 'M' else w_teams_info_df\n            # There is no reason for setting team names or probabilities if these matchups will never happen in the tournament\n            team_a_name = None # set_team_name(team_a, teams_info_df)\n            team_b_name = None # set_team_name(team_b, teams_info_df)\n\n            game_id = f\"{season}_{team_a}_{team_b}\"\n            data.append([game_id, team_a, team_b, team_a_name, team_b_name, 0.5])\n\n    data_df = pd.DataFrame(data, columns=['ID', 'TeamA', 'TeamB', 'TeamNameA', 'TeamNameB', 'Pred'])\n    return data_df\n\ncurrent_szn_df = pd.concat([m_current_szn_df, w_current_szn_df], axis=0).reset_index(drop=True)\ncurrent_szn_df['ID'] = current_szn_df.apply(lambda row: set_game_id(row), axis=1)\n\nm_impossible_df = set_impossible_games_probs('M', m_current_szn_df, m_teams_df)\nw_impossible_df = set_impossible_games_probs('W', w_current_szn_df, w_teams_df)\n\nfinal_df = current_szn_df.loc[:, ['ID', 'TeamA', 'TeamB', 'TeamNameA', 'TeamNameB', 'AProb']].rename({'AProb': 'Pred'}, axis=1)\nfinal_df = pd.concat([final_df, m_impossible_df, w_impossible_df], axis=0).sort_values(by='ID').reset_index(drop=True)\n\ndisplay(final_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_file(df):\n    df_filtered = df.loc[:, ['ID', 'Pred']]\n    df_filtered.to_csv('submission.csv', index=False)\n    \ncreate_submission_file(final_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rf_best_parameters(gender):\n    teams_df, reg_szn_df, tourn_df, seed_df = get_files_by_competition(gender)\n\n    data_df = build_dataset(gender, start_data_season[gender], reg_szn_df, tourn_df, seed_df, end_season = 2022) \n    x = data_df.drop(['TeamNameA', 'TeamNameB', 'Winner'], axis=1)\n    y = data_df.loc[:, 'Winner']\n\n    from sklearn.model_selection import RandomizedSearchCV\n    rs_optimizer = RandomForestClassifier()\n\n    # Number of trees in random forest\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\n    max_features = ['sqrt', 'log2', None]\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10]\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 4]\n    # Method of selecting samples for training each tree\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                    'max_features': max_features,\n                    'max_depth': max_depth,\n                    'min_samples_split': min_samples_split,\n                    'min_samples_leaf': min_samples_leaf,\n                    'bootstrap': bootstrap}\n\n    rf_random = RandomizedSearchCV(estimator = rs_optimizer, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n\n    rf_random.fit(x, y)\n\n    best_random = rf_random.best_estimator_\n    best_parameters = rf_random.cv_results_\n    #     print(best_parameters)\n    print(best_random)\n    \n# get_rf_best_parameters('M')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}